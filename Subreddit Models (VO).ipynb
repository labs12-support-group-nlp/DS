{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "uim4Uu_nIcNW",
    "outputId": "c5956387-a7b4-4464-f8fc-a9008d460c7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.</td>\n",
       "      <td>day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm finally okay with what I lost.</td>\n",
       "      <td>-pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!</td>\n",
       "      <td>that be all thank -pron- for -pron- time</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I told my boss to fuck off and quit my job all before 8am.</td>\n",
       "      <td>-pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>FUCK YEA</td>\n",
       "      <td>its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                   title  \\\n",
       "0  Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.   \n",
       "1                                                                                                     I'm finally okay with what I lost.   \n",
       "2                                                         VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!   \n",
       "3                                                                             I told my boss to fuck off and quit my job all before 8am.   \n",
       "4                                                                                                                               FUCK YEA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0  day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...   \n",
       "1  -pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...   \n",
       "2                                                                                                                                                                                                                                                                     that be all thank -pron- for -pron- time   \n",
       "3  -pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...   \n",
       "4                                                                                                                                                                                   its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine   \n",
       "\n",
       "    subreddit  is_self  \n",
       "0  offmychest     True  \n",
       "1  offmychest     True  \n",
       "2  offmychest     True  \n",
       "3  offmychest     True  \n",
       "4  offmychest     True  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "df = pd.read_csv('subreddits_mental_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "1Wg4eXnBIcN1",
    "outputId": "58caa955-3b44-402d-ccb6-9bb1f8300d4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.</td>\n",
       "      <td>day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm finally okay with what I lost.</td>\n",
       "      <td>-pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!</td>\n",
       "      <td>that be all thank -pron- for -pron- time</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I told my boss to fuck off and quit my job all before 8am.</td>\n",
       "      <td>-pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FUCK YEA</td>\n",
       "      <td>its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   title  \\\n",
       "0  Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.   \n",
       "1                                                                                                     I'm finally okay with what I lost.   \n",
       "2                                                         VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!   \n",
       "3                                                                             I told my boss to fuck off and quit my job all before 8am.   \n",
       "4                                                                                                                               FUCK YEA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0  day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...   \n",
       "1  -pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...   \n",
       "2                                                                                                                                                                                                                                                                     that be all thank -pron- for -pron- time   \n",
       "3  -pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...   \n",
       "4                                                                                                                                                                                   its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine   \n",
       "\n",
       "    subreddit  is_self  \n",
       "0  offmychest     True  \n",
       "1  offmychest     True  \n",
       "2  offmychest     True  \n",
       "3  offmychest     True  \n",
       "4  offmychest     True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "yng5TzrtIcN-",
    "outputId": "16de87a7-46df-402c-d92e-396187804124"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8001</td>\n",
       "      <td>I've noticed a lot of you have significant others/spouses. How were you able to meet them while being agoraphobic?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8002</td>\n",
       "      <td>Grocery store Friday....</td>\n",
       "      <td>with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8003</td>\n",
       "      <td>Disability Barrier</td>\n",
       "      <td>-pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8004</td>\n",
       "      <td>Gonna be a big day.</td>\n",
       "      <td>grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8005</td>\n",
       "      <td>I don't leave my apartment</td>\n",
       "      <td>-pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0        8001   \n",
       "1        8002   \n",
       "2        8003   \n",
       "3        8004   \n",
       "4        8005   \n",
       "\n",
       "                                                                                                                title  \\\n",
       "0  I've noticed a lot of you have significant others/spouses. How were you able to meet them while being agoraphobic?   \n",
       "1                                                                                            Grocery store Friday....   \n",
       "2                                                                                                  Disability Barrier   \n",
       "3                                                                                                 Gonna be a big day.   \n",
       "4                                                                                          I don't leave my apartment   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0                                                                                                                                                                                                                                                                                                          NaN   \n",
       "1                                                                                                                                                              with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already   \n",
       "2  -pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...   \n",
       "3                                                                    grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck   \n",
       "4                                                                                                                                           -pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour   \n",
       "\n",
       "     subreddit  is_self  \n",
       "0  Agoraphobia     True  \n",
       "1  Agoraphobia     True  \n",
       "2  Agoraphobia     True  \n",
       "3  Agoraphobia     True  \n",
       "4  Agoraphobia     True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('subreddits_mental_cleaned1.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "xhpkC-2jIcOJ",
    "outputId": "4cb2d8b0-afae-47b0-98b7-831a8eb71df2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've noticed a lot of you have significant others/spouses. How were you able to meet them while being agoraphobic?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grocery store Friday....</td>\n",
       "      <td>with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disability Barrier</td>\n",
       "      <td>-pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gonna be a big day.</td>\n",
       "      <td>grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't leave my apartment</td>\n",
       "      <td>-pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                title  \\\n",
       "0  I've noticed a lot of you have significant others/spouses. How were you able to meet them while being agoraphobic?   \n",
       "1                                                                                            Grocery store Friday....   \n",
       "2                                                                                                  Disability Barrier   \n",
       "3                                                                                                 Gonna be a big day.   \n",
       "4                                                                                          I don't leave my apartment   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0                                                                                                                                                                                                                                                                                                          NaN   \n",
       "1                                                                                                                                                              with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already   \n",
       "2  -pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...   \n",
       "3                                                                    grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck   \n",
       "4                                                                                                                                           -pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour   \n",
       "\n",
       "     subreddit  is_self  \n",
       "0  Agoraphobia     True  \n",
       "1  Agoraphobia     True  \n",
       "2  Agoraphobia     True  \n",
       "3  Agoraphobia     True  \n",
       "4  Agoraphobia     True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop('Unnamed: 0', axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "jyK65cJ9IcOS",
    "outputId": "db9f124a-9b9f-43d9-b653-8cc064886110"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "selftext     676\n",
       "subreddit      0\n",
       "is_self        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "YZaePkQ4IcOc",
    "outputId": "23d1c176-8532-4646-a778-4bd470c454c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "selftext     1253\n",
       "subreddit       0\n",
       "is_self         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "yywsmTFgIcOn",
    "outputId": "e4eba295-17bb-4335-e222-5734df1cbb10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "is_self      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['selftext'])\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "fhJ6DZDvIcO1",
    "outputId": "6b9f1e04-d723-493f-9093-fe165b537247"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "is_self      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.dropna(subset=['selftext'])\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "gpeCTvifIcPF",
    "outputId": "45c52cf2-5180-444b-ef57-89562fce1a1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grocery store Friday....</td>\n",
       "      <td>with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disability Barrier</td>\n",
       "      <td>-pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gonna be a big day.</td>\n",
       "      <td>grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't leave my apartment</td>\n",
       "      <td>-pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Going to a concert alone...</td>\n",
       "      <td>one of -pron- favorite artist be come to -pron- city next month and -pron- really want to go however .. the ticket be bit pricy and -pron- would only be able to afford one -pron- at pretty big arena ~15,000 people but -pron- have be there before and be pretty familiar with -pron- -pron- really d...</td>\n",
       "      <td>Agoraphobia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title  \\\n",
       "1     Grocery store Friday....   \n",
       "2           Disability Barrier   \n",
       "3          Gonna be a big day.   \n",
       "4   I don't leave my apartment   \n",
       "5  Going to a concert alone...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "1                                                                                                                                                              with -pron- husband -pron- have plan on go inside the grocery store -pron- have be sit outside while -pron- go in -pron- be bit anxious already   \n",
       "2  -pron- have have anxiety relate to healthcare for the last decade however -pron- agoraphobia begin about year and half ago after major incident occur inside hospital -pron- do not want to go back -pron- be not go back -pron- do not trust doctor and -pron- be constantly scared of end up in one -p...   \n",
       "3                                                                    grocery shop by -pron- compound with have to drive there ... i’m terrify of drive and -pron- be recover from an ed so ... this will be unpleasant then walk over mile in the busy city to and from doctor ’s appointment wish -pron- luck   \n",
       "4                                                                                                                                           -pron- leave the house maybe once week to go to -pron- therapy appointment be -pron- bad or unhealthy to not leave the house more than once week for only few hour   \n",
       "5  one of -pron- favorite artist be come to -pron- city next month and -pron- really want to go however .. the ticket be bit pricy and -pron- would only be able to afford one -pron- at pretty big arena ~15,000 people but -pron- have be there before and be pretty familiar with -pron- -pron- really d...   \n",
       "\n",
       "     subreddit  is_self  \n",
       "1  Agoraphobia     True  \n",
       "2  Agoraphobia     True  \n",
       "3  Agoraphobia     True  \n",
       "4  Agoraphobia     True  \n",
       "5  Agoraphobia     True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "UgPwSXhgIcPN",
    "outputId": "0fc5583b-47ec-49b6-d79c-fbce2ce77f74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.</td>\n",
       "      <td>day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm finally okay with what I lost.</td>\n",
       "      <td>-pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!</td>\n",
       "      <td>that be all thank -pron- for -pron- time</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I told my boss to fuck off and quit my job all before 8am.</td>\n",
       "      <td>-pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FUCK YEA</td>\n",
       "      <td>its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   title  \\\n",
       "0  Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.   \n",
       "1                                                                                                     I'm finally okay with what I lost.   \n",
       "2                                                         VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!   \n",
       "3                                                                             I told my boss to fuck off and quit my job all before 8am.   \n",
       "4                                                                                                                               FUCK YEA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0  day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...   \n",
       "1  -pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...   \n",
       "2                                                                                                                                                                                                                                                                     that be all thank -pron- for -pron- time   \n",
       "3  -pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...   \n",
       "4                                                                                                                                                                                   its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine   \n",
       "\n",
       "    subreddit  is_self  \n",
       "0  offmychest     True  \n",
       "1  offmychest     True  \n",
       "2  offmychest     True  \n",
       "3  offmychest     True  \n",
       "4  offmychest     True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg = pd.concat([df, df2], ignore_index= \"true\")\n",
    "sg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "nTKbK290IcPa",
    "outputId": "9e1c3d81-a95a-4764-f2f3-d2aa97f90459"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>Learning about anxiety</td>\n",
       "      <td>-pron- do not suffer with anxiety -pron- girlfriend do before -pron- tell -pron- about anxiety -pron- know nothing and now -pron- want to be an expert -pron- think that know more about -pron- would help -pron- to help -pron- ... but just ask -pron- be not the easy way because -pron- find -pron- ...</td>\n",
       "      <td>anxietysupporters</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>Let's (re)introduce ourselves</td>\n",
       "      <td>-pron- guess if -pron- be go to try to revive this subreddit -pron- may as well get to know each other bit -pron- girlfriend suffer from depression and panic attack that seem to be root in an emotionally abusive childhood -pron- be really hard to know what may provoke an episode and after year o...</td>\n",
       "      <td>anxietysupporters</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>Is this sub-reddit still active?</td>\n",
       "      <td>-pron- have be look for place like this ever since -pron- start date -pron- girlfriend year ago be -pron- still active or should -pron- continue -pron- search</td>\n",
       "      <td>anxietysupporters</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>Let's introduce ourselves</td>\n",
       "      <td>hi everybody -pron- think -pron- may be good idea to briefly introduce -pron- and -pron- relationship with people who have anxiety -pron- be in -pron- mid twenty and have be marry to -pron- wonderful wife for year shortly after -pron- get marry -pron- begin experience severe panic attack and -pr...</td>\n",
       "      <td>anxietysupporters</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>Any positive accounts of overcoming anxiety?</td>\n",
       "      <td>-pron- wife have anxiety and want to read more people 's experience however -pron- only seem to find account where the people just get bad and bad and suffer terribly -pron- would like to provide -pron- with some account of people who get through -pron- and be live happy positive productive life...</td>\n",
       "      <td>anxietysupporters</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "14055                        Learning about anxiety   \n",
       "14056                 Let's (re)introduce ourselves   \n",
       "14057              Is this sub-reddit still active?   \n",
       "14058                     Let's introduce ourselves   \n",
       "14059  Any positive accounts of overcoming anxiety?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          selftext  \\\n",
       "14055  -pron- do not suffer with anxiety -pron- girlfriend do before -pron- tell -pron- about anxiety -pron- know nothing and now -pron- want to be an expert -pron- think that know more about -pron- would help -pron- to help -pron- ... but just ask -pron- be not the easy way because -pron- find -pron- ...   \n",
       "14056  -pron- guess if -pron- be go to try to revive this subreddit -pron- may as well get to know each other bit -pron- girlfriend suffer from depression and panic attack that seem to be root in an emotionally abusive childhood -pron- be really hard to know what may provoke an episode and after year o...   \n",
       "14057                                                                                                                                               -pron- have be look for place like this ever since -pron- start date -pron- girlfriend year ago be -pron- still active or should -pron- continue -pron- search   \n",
       "14058  hi everybody -pron- think -pron- may be good idea to briefly introduce -pron- and -pron- relationship with people who have anxiety -pron- be in -pron- mid twenty and have be marry to -pron- wonderful wife for year shortly after -pron- get marry -pron- begin experience severe panic attack and -pr...   \n",
       "14059  -pron- wife have anxiety and want to read more people 's experience however -pron- only seem to find account where the people just get bad and bad and suffer terribly -pron- would like to provide -pron- with some account of people who get through -pron- and be live happy positive productive life...   \n",
       "\n",
       "               subreddit  is_self  \n",
       "14055  anxietysupporters     True  \n",
       "14056  anxietysupporters     True  \n",
       "14057  anxietysupporters     True  \n",
       "14058  anxietysupporters     True  \n",
       "14059  anxietysupporters     True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "XUqaz2CJIcPi",
    "outputId": "321a1cb0-899c-4cc8-fce6-bb93ae40bba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlAnon                 984\n",
       "Anger                  974\n",
       "Anxiety                968\n",
       "offmychest             942\n",
       "alcoholicsanonymous    939\n",
       "ADHD                   929\n",
       "alcoholism             925\n",
       "Agoraphobia            917\n",
       "leaves                 912\n",
       "rapecounseling         910\n",
       "addiction              904\n",
       "Advice                 888\n",
       "cripplingalcoholism    792\n",
       "Anxietyhelp            688\n",
       "Antipsychiatry         443\n",
       "ADD                    278\n",
       "MenGetRapedToo         262\n",
       "afterthesilence        166\n",
       "anxietysuccess         112\n",
       "affirmations            79\n",
       "Disorder                30\n",
       "anxietysupporters       18\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "bTP5jqawIcPn",
    "outputId": "41864f96-3742-4384-aeca-e8ce29f5a081"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['offmychest', 'ADD', 'cripplingalcoholism', 'Disorder', 'leaves',\n",
       "       'MenGetRapedToo', 'rapecounseling', 'addiction', 'ADHD', 'Advice',\n",
       "       'affirmations', 'afterthesilence', 'Agoraphobia', 'AlAnon',\n",
       "       'alcoholicsanonymous', 'alcoholism', 'Anger', 'Antipsychiatry',\n",
       "       'Anxiety', 'Anxietyhelp', 'anxietysuccess', 'anxietysupporters'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.subreddit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "YrYOQ_X-IcPw",
    "outputId": "6201e7d9-c141-4a0a-ee17-bc4b7227808c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14060</td>\n",
       "      <td>14060</td>\n",
       "      <td>13933</td>\n",
       "      <td>14060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13773</td>\n",
       "      <td>13989</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Help</td>\n",
       "      <td>share -pron- rant and thought here also see full past discussion here](https://www.reddit.com addiction search?q friday+lounge&amp;restrict_sr on&amp;sort relevance&amp;t all</td>\n",
       "      <td>AlAnon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>984</td>\n",
       "      <td>14060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title  \\\n",
       "count   14060   \n",
       "unique  13773   \n",
       "top      Help   \n",
       "freq       18   \n",
       "\n",
       "                                                                                                                                                                  selftext  \\\n",
       "count                                                                                                                                                                14060   \n",
       "unique                                                                                                                                                               13989   \n",
       "top     share -pron- rant and thought here also see full past discussion here](https://www.reddit.com addiction search?q friday+lounge&restrict_sr on&sort relevance&t all   \n",
       "freq                                                                                                                                                                     9   \n",
       "\n",
       "       subreddit is_self  \n",
       "count      13933   14060  \n",
       "unique        19       1  \n",
       "top       AlAnon    True  \n",
       "freq         984   14060  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "value_counts = sg.subreddit.value_counts() # Specific column \n",
    "to_remove = value_counts[value_counts < 100].index\n",
    "sg.subreddit.replace(to_remove, np.nan, inplace=True)\n",
    "sg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "1MAXOi1ZIcP8",
    "outputId": "a3110c04-0246-415b-8b93-5887f8bd8297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "selftext       0\n",
       "subreddit    127\n",
       "is_self        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "BNVDxXkbIcQF",
    "outputId": "7d9c15c2-1357-4ef1-8f93-10a1deae4dd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "is_self      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg = sg.dropna()\n",
    "sg.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "ItXASCx8IcQY",
    "outputId": "4e05640b-893b-4447-ddc0-28cd851546fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlAnon                 984\n",
       "Anger                  974\n",
       "Anxiety                968\n",
       "offmychest             942\n",
       "alcoholicsanonymous    939\n",
       "ADHD                   929\n",
       "alcoholism             925\n",
       "Agoraphobia            917\n",
       "leaves                 912\n",
       "rapecounseling         910\n",
       "addiction              904\n",
       "Advice                 888\n",
       "cripplingalcoholism    792\n",
       "Anxietyhelp            688\n",
       "Antipsychiatry         443\n",
       "ADD                    278\n",
       "MenGetRapedToo         262\n",
       "afterthesilence        166\n",
       "anxietysuccess         112\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mssMHcGEIcQk",
    "outputId": "f7298fc0-cc0e-48ca-e88f-8203033cce02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13933"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8BOoZNOhIcQq",
    "outputId": "9cb9ed99-c672-469a-e06d-2d20c7a220e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07062369913155817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy if guessing majority class\n",
    "984/13933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "enZ95rJzIcQy",
    "outputId": "24a38c71-578b-46eb-9723-8c1eb68b9d84"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.</td>\n",
       "      <td>day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm finally okay with what I lost.</td>\n",
       "      <td>-pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!</td>\n",
       "      <td>that be all thank -pron- for -pron- time</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I told my boss to fuck off and quit my job all before 8am.</td>\n",
       "      <td>-pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FUCK YEA</td>\n",
       "      <td>its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   title  \\\n",
       "0  Please be wary of people asking for donations. We have a scammer that makes their rounds every so often and she's duped a few of you.   \n",
       "1                                                                                                     I'm finally okay with what I lost.   \n",
       "2                                                         VACCINES ARE GOOD! THEY SAVE LIVES! PLEASE GET THEM FOR YOU AND YOUR CHILDREN!   \n",
       "3                                                                             I told my boss to fuck off and quit my job all before 8am.   \n",
       "4                                                                                                                               FUCK YEA   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      selftext  \\\n",
       "0  day or two ago post pop up by girl claim -pron- be incredibly ill bedridden and that -pron- boyfriend have tamper with -pron- birth control -pron- claim -pron- need an abortion -pron- wonderful helpful people try to direct -pron- to abortion fund hotline anything that could help -pron- say -pron...   \n",
       "1  -pron- have -pron- daughter at sixteen -pron- be now in college year and half later and single dad lose freedom be the hard part for -pron- -pron- could not hang out with -pron- friend listen to shitty music sleep full night study when -pron- want shit -pron- still can not take shower without ba...   \n",
       "2                                                                                                                                                                                                                                                                     that be all thank -pron- for -pron- time   \n",
       "3  -pron- have be check -pron- hour religiously -pron- notice some miss hour last pay cycle and have -pron- fix -pron- but be still little sketch and worried that -pron- may mess up again -pron- fiancé and -pron- be leave for portland in two day and -pron- can not afford for -pron- to make another ...   \n",
       "4                                                                                                                                                                                   its currently 5:31 am and -pron- just finish my 14 page research paper yaaay and im not even tired wahoooo the day be mine   \n",
       "\n",
       "    subreddit  is_self  category_id  \n",
       "0  offmychest     True            0  \n",
       "1  offmychest     True            0  \n",
       "2  offmychest     True            0  \n",
       "3  offmychest     True            0  \n",
       "4  offmychest     True            0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg['category_id'] = sg['subreddit'].factorize()[0]\n",
    "sg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "ypb4B8jvIcQ5",
    "outputId": "66384b01-3fc7-4781-ec74-58f5decf12da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10613</th>\n",
       "      <td>Tried to make a dry week. Failed at day 2.</td>\n",
       "      <td>after fight with -pron- mother -pron- get so angry at -pron- that -pron- think -pron- deserve to be fuck and go out and buy high% alcohol</td>\n",
       "      <td>alcoholism</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>Graduation Research Project</td>\n",
       "      <td>hello x200b -pron- be senior from lourdes university as part of -pron- program -pron- need to conduct research project on -pron- own -pron- have choose to investigate the correlation between mixed parenting style and the development of addictive behavior this come in the form of quick survey tha...</td>\n",
       "      <td>addiction</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10442</th>\n",
       "      <td>My father is an alcoholic and I'm pretty lost...</td>\n",
       "      <td>hey there!me and -pron- mom just discover that -pron- father be an alcoholic for sure -pron- have problem with drinking since some time but -pron- be only beer and year ago -pron- promise -pron- mom -pron- will be all right and that -pron- will stop drink few day ago -pron- mom catch -pron- drin...</td>\n",
       "      <td>alcoholism</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10742</th>\n",
       "      <td>My[29F] BF's[31M] drinking is getting out of control, and now there's solo drug use. Scared and need guidance on how to approach.</td>\n",
       "      <td>tl;dr -pron- boyfriend 's drinking be get progressively more and more out of control to an alarming degree -pron- be do amphetamine alone twice this week something -pron- have ever see -pron- do before need guidance on how to approach this in way -pron- will take seriously hi friend -pron- boyfr...</td>\n",
       "      <td>alcoholism</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>[28M] my gf [27F] use to be alcoholic, after a year of being sober, she start to drink alcohol and i am scared</td>\n",
       "      <td>-pron- use to be very bad alcoholic to the point -pron- do not know who be -pron- where be -pron- almost everyday after devastating thing happen year ago -pron- decide to stop drink and -pron- start to go to aa and see professional for help for couple of month -pron- have be sober for little bit...</td>\n",
       "      <td>alcoholism</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   title  \\\n",
       "10613                                                                                         Tried to make a dry week. Failed at day 2.   \n",
       "4565                                                                                                         Graduation Research Project   \n",
       "10442                                                                                   My father is an alcoholic and I'm pretty lost...   \n",
       "10742  My[29F] BF's[31M] drinking is getting out of control, and now there's solo drug use. Scared and need guidance on how to approach.   \n",
       "10765                     [28M] my gf [27F] use to be alcoholic, after a year of being sober, she start to drink alcohol and i am scared   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          selftext  \\\n",
       "10613                                                                                                                                                                    after fight with -pron- mother -pron- get so angry at -pron- that -pron- think -pron- deserve to be fuck and go out and buy high% alcohol   \n",
       "4565   hello x200b -pron- be senior from lourdes university as part of -pron- program -pron- need to conduct research project on -pron- own -pron- have choose to investigate the correlation between mixed parenting style and the development of addictive behavior this come in the form of quick survey tha...   \n",
       "10442  hey there!me and -pron- mom just discover that -pron- father be an alcoholic for sure -pron- have problem with drinking since some time but -pron- be only beer and year ago -pron- promise -pron- mom -pron- will be all right and that -pron- will stop drink few day ago -pron- mom catch -pron- drin...   \n",
       "10742  tl;dr -pron- boyfriend 's drinking be get progressively more and more out of control to an alarming degree -pron- be do amphetamine alone twice this week something -pron- have ever see -pron- do before need guidance on how to approach this in way -pron- will take seriously hi friend -pron- boyfr...   \n",
       "10765  -pron- use to be very bad alcoholic to the point -pron- do not know who be -pron- where be -pron- almost everyday after devastating thing happen year ago -pron- decide to stop drink and -pron- start to go to aa and see professional for help for couple of month -pron- have be sober for little bit...   \n",
       "\n",
       "        subreddit  is_self  category_id  \n",
       "10613  alcoholism     True           13  \n",
       "4565    addiction     True            6  \n",
       "10442  alcoholism     True           13  \n",
       "10742  alcoholism     True           13  \n",
       "10765  alcoholism     True           13  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ap9SuBBtIcRF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = sg.selftext\n",
    "y = sg.category_id\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Bio3xS5MIcRI",
    "outputId": "e112cd87-957a-4d36-e263-391e41e88e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11146,)\n",
      "(2787,)\n",
      "(11146,)\n",
      "(2787,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UctNc7kLIcRN",
    "outputId": "ef689e2b-b1c2-4f6c-e33c-42c37d61175b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11146, 25957)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "68QhB7_WIcRX",
    "outputId": "b99fa2a9-1158-43c3-88fb-f3178a800eef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11146, 25957)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF: term-frequency times inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMd7AIyWIcRd"
   },
   "outputs": [],
   "source": [
    "# Classifier using Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpQhnb2UIcRh"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "g9ne5dK0IcRm",
    "outputId": "b6bbd75b-7743-45d1-d885-a48be6815ec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4406171510584858"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAZtBhaCIcRt"
   },
   "source": [
    "Somewhat decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "Ru_CWMNcIcRv",
    "outputId": "31e9c85f-c788-433a-968c-da16002d0cbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valerie\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5855758880516685"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# support vector machine classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, max_iter=5, tol=None, random_state=42)),\n",
    "])\n",
    "\n",
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiEcoYjAIcR3"
   },
   "source": [
    "Better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zofrU1XOIcR4"
   },
   "outputs": [],
   "source": [
    "# Tuning hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EbH3ihyIcSA"
   },
   "outputs": [],
   "source": [
    "# Grid search with Naive Bayes\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "kwPKwurIIcSC",
    "outputId": "173af07e-7ffa-4bae-ec29-965d67e036d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valerie\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.540732101202225\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(gs_clf.best_score_)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JK5NKcjNIcSI"
   },
   "source": [
    "Improved from 44% to 54% over Naive Bayes without tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P4U7JAeIcSJ"
   },
   "outputs": [],
   "source": [
    "# Grid search with SVM classifier\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf-svm__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "CJAUDWK3IcSP",
    "outputId": "8360986a-d435-462f-99d0-e16f233ff1f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valerie\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Valerie\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5806567378431724\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDxnQVa3IcSV"
   },
   "source": [
    "The SVM classifier with hypertuning performed about as well as it did without hypertuning: 58% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "A7p3vz2OIcSX",
    "outputId": "eb591cc5-6fd3-471e-cd65-973e73f9c00a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valerie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "8KhbVXZBIcSc",
    "outputId": "2b33d068-579e-40ea-90b7-3574edc3925b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ztUPgeoBIcSf",
    "outputId": "ee8a7f69-4a7e-43c8-9aca-f963cc66fd8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285252960172229"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(X_test)\n",
    "np.mean(predicted_mnb_stemmed == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICdbvl5ZIcSi"
   },
   "source": [
    "Naive Bayes is about 53% accurate after stemming and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS9yrewyIcSi"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGQj4kIDIcSl"
   },
   "outputs": [],
   "source": [
    "vectorizers = [\n",
    "    TfidfVectorizer(stop_words='english',\n",
    "                    max_features=None),\n",
    "    CountVectorizer(stop_words='english',\n",
    "                   max_features=None)\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(),\n",
    "    LinearSVC(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "clf_names = [\n",
    "         \"Naive Bayes\",\n",
    "         \"Linear SVC\",\n",
    "         \"Logistic Regression\",\n",
    "         \"Random Forest\",\n",
    "        ]\n",
    "\n",
    "vect_names = [\n",
    "    \"TfidfVectorizer\",\n",
    "    \"CountVectorizer\"\n",
    "]\n",
    "\n",
    "clf_params = [\n",
    "              {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__alpha': (1e-2, 1e-3)},\n",
    "              {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__C': (np.logspace(-5, 1, 5))},\n",
    "              {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__C': (np.logspace(-5, 1, 5))},\n",
    "              {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf__max_depth': (1, 2)},\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2726
    },
    "colab_type": "code",
    "id": "SKbKBg4_IcSm",
    "outputId": "a88bae1d-ea17-4c28-a3d3-823f787acc8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Naive Bayes\n",
      "Vectorizer: TfidfVectorizer\n",
      "Score: 0.5343\n",
      "Params: {'clf__alpha': 0.01, 'vect__ngram_range': (1, 1)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Naive Bayes\n",
      "Vectorizer: CountVectorizer\n",
      "Score: 0.5430\n",
      "Params: {'clf__alpha': 0.01, 'vect__ngram_range': (1, 2)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   55.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Linear SVC\n",
      "Vectorizer: TfidfVectorizer\n",
      "Score: 0.6157\n",
      "Params: {'clf__C': 0.31622776601683794, 'vect__ngram_range': (1, 2)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 25.7min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Linear SVC\n",
      "Vectorizer: CountVectorizer\n",
      "Score: 0.5838\n",
      "Params: {'clf__C': 0.01, 'vect__ngram_range': (1, 2)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.9min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Vectorizer: TfidfVectorizer\n",
      "Score: 0.6106\n",
      "Params: {'clf__C': 10.0, 'vect__ngram_range': (1, 2)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 30.2min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Logistic Regression\n",
      "Vectorizer: CountVectorizer\n",
      "Score: 0.5858\n",
      "Params: {'clf__C': 0.01, 'vect__ngram_range': (1, 2)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.7min finished\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Random Forest\n",
      "Vectorizer: TfidfVectorizer\n",
      "Score: 0.1766\n",
      "Params: {'clf__max_depth': 2, 'vect__ngram_range': (1, 1)}\n",
      "------------------------------\n",
      "            \n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Random Forest\n",
      "Vectorizer: CountVectorizer\n",
      "Score: 0.1770\n",
      "Params: {'clf__max_depth': 2, 'vect__ngram_range': (1, 1)}\n",
      "------------------------------\n",
      "            \n",
      "CPU times: user 4min 1s, sys: 3.14 s, total: 4min 4s\n",
      "Wall time: 1h 23min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = []\n",
    "for classifier, clf_name, params in zip(classifiers, \n",
    "                                        clf_names, \n",
    "                                        clf_params):\n",
    "    for vectorizer, vect_name in zip(vectorizers, \n",
    "                                     vect_names):\n",
    "        pipe = Pipeline([\n",
    "            ('vect', vectorizer),\n",
    "            ('clf', classifier),\n",
    "        ])\n",
    "        gs = GridSearchCV(pipe, \n",
    "                          param_grid=params, \n",
    "                          n_jobs=-1,\n",
    "                          scoring='accuracy',\n",
    "                          cv=5,\n",
    "                          verbose=10)\n",
    "        \n",
    "        gs.fit(sg.selftext, sg.category_id)\n",
    "        score = gs.best_score_\n",
    "        print(f'''\n",
    "Classifier: {clf_name}\n",
    "Vectorizer: {vect_name}\n",
    "Score: {gs.best_score_:.4f}\n",
    "Params: {gs.best_params_}\n",
    "------------------------------\n",
    "            ''')\n",
    "        models.append((clf_name, vect_name, gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "a-ABsMQwIcSs",
    "outputId": "1fcd44a2-5d4f-4a41-ac90-fc4b6ae878ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the winner is...\n",
      "\n",
      "Classifier: Linear SVC\n",
      "Vectorizer: TfidfVectorizer\n",
      "Score: 0.6157324337902821\n",
      "Params: {'clf__C': 0.31622776601683794, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "models = sorted(models, key=lambda tup: tup[2])\n",
    "print('And the winner is...')\n",
    "print()\n",
    "print('Classifier:', models[-1][0])\n",
    "print('Vectorizer:', models[-1][1])\n",
    "print('Score:', models[-1][2])\n",
    "print('Params:', models[-1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVC in module sklearn.svm.classes:\n",
      "\n",
      "class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  Linear Support Vector Classification.\n",
      " |  \n",
      " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input and the multiclass support\n",
      " |  is handled according to a one-vs-the-rest scheme.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
      " |      Specifies the norm used in the penalization. The 'l2'\n",
      " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      " |      vectors that are sparse.\n",
      " |  \n",
      " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
      " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      " |      square of the hinge loss.\n",
      " |  \n",
      " |  dual : bool, (default=True)\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-4)\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
      " |      Determines the multi-class strategy if `y` contains more than\n",
      " |      two classes.\n",
      " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      " |      While `crammer_singer` is interesting from a theoretical perspective\n",
      " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      " |      to better accuracy and is more expensive to compute.\n",
      " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      " |      will be ignored.\n",
      " |  \n",
      " |  fit_intercept : boolean, optional (default=True)\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, optional (default=1)\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      ``[x, self.intercept_scaling]``,\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : int, (default=0)\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data for the dual coordinate descent (if ``dual=True``). When\n",
      " |      ``dual=False`` the underlying implementation of :class:`LinearSVC`\n",
      " |      is not random and ``random_state`` has no effect on the results. If\n",
      " |      int, random_state is the seed used by the random number generator; If\n",
      " |      RandomState instance, random_state is the random number generator; If\n",
      " |      None, the random number generator is the RandomState instance used by\n",
      " |      `np.random`.\n",
      " |  \n",
      " |  max_iter : int, (default=1000)\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      " |  >>> clf = LinearSVC(random_state=0, tol=1e-5)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      " |       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      " |       multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
      " |  >>> print(clf.coef_)\n",
      " |  [[0.085... 0.394... 0.498... 0.375...]]\n",
      " |  >>> print(clf.intercept_)\n",
      " |  [0.284...]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller ``tol`` parameter.\n",
      " |  \n",
      " |  The underlying implementation, liblinear, uses a sparse internal\n",
      " |  representation for the data that will incur a memory copy.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  `LIBLINEAR: A Library for Large Linear Classification\n",
      " |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVC\n",
      " |      Implementation of Support Vector Machine classifier using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |      Furthermore SVC multi-class mode is implemented using one\n",
      " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      " |      possible to implement one vs the rest with SVC by using the\n",
      " |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      " |  \n",
      " |      Finally SVC can fit dense data without memory copy if the input\n",
      " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDClassifier\n",
      " |      SGDClassifier can optimize the same cost function as LinearSVC\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVC\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples in the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target vector relative to X\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r49RtEZUIcSw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5862935055615357"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([('vect', TfidfVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', LinearSVC(random_state=42)),\n",
    "])\n",
    "\n",
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with SVM classifier\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'clf-svm__C': (np.logspace(-3, 1, 5))\n",
    "}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valerie\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6145702494168311\n",
      "{'clf-svm__C': 1.0, 'vect__ngram_range': (1, 2)}\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class TfidfVectorizer(CountVectorizer)\n",
      " |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      " |  \n",
      " |  Equivalent to CountVectorizer followed by TfidfTransformer.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : boolean, default True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      " |      list is returned. 'english' is currently the only supported string\n",
      " |      value.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non-zero term counts are set to 1. This does not mean\n",
      " |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      " |      is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  norm : 'l1', 'l2' or None, optional\n",
      " |      Norm used to normalize term vectors. None for no normalization.\n",
      " |  \n",
      " |  use_idf : boolean, default=True\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : boolean, default=True\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : boolean, default=False\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  idf_ : array, shape (n_features)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if  ``use_idf`` is True.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = TfidfVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.shape)\n",
      " |  (4, 9)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  CountVectorizer\n",
      " |      Tokenize the documents and count the occurrences of token and return\n",
      " |      them as a sparse matrix\n",
      " |  \n",
      " |  TfidfTransformer\n",
      " |      Apply Term Frequency Inverse Document Frequency normalization to a\n",
      " |      sparse matrix of occurrence counts.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfVectorizer\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf from training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : TfidfVectorizer\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf, return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  transform(self, raw_documents, copy=True)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      " |      fit_transform).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      copy : boolean, default True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  norm\n",
      " |  \n",
      " |  smooth_idf\n",
      " |  \n",
      " |  sublinear_tf\n",
      " |  \n",
      " |  use_idf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CountVectorizer:\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : string\n",
      " |          The string to decode\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63329745245784"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([('vect', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', LinearSVC(C=1.0, random_state=42)),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'sg_model.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Subreddit Models (VO).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
