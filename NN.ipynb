{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/zarrina/Desktop/flask/exploration/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zarrina/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe, an unsupervised learning algorithm for obtaining vector representations for words\n",
    "wget.download('http://www-nlp.stanford.edu/data/glove.840B.300d.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word2vec pre-trained Google News corpus\n",
    "wget.download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions strings to lower case and eliminate stop words\n",
    "# apply gensim word2vec model trained on Google News corpus\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions strings to lower case and eliminate stop words\n",
    "# apply gensim word2vec model trained on Google News corpus\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions strings to lower case and eliminate stop words\n",
    "# apply gensim word2vec model trained Google News corpus with precomputed L2-normalized vectors.\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        M.append(model[w])  \n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id columns\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build word2vec model and appy to text strings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "data['wmd'] = data.apply(lambda x: (x['full_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec model, normalize vectors, and appy to text strings\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "data['norm_wmd'] = data.apply(lambda x: (x['full_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_vectors = np.zeros(data.shape[1], 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply sent2vec function to question1_vectors and question2_vectors\n",
    "for i, q in tqdm(enumerate(data.full_text.values.ravel)):\n",
    "    full_text_vectors[i] = sent2vec(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>group</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please be wary of people asking for donations ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>1509</td>\n",
       "      <td>274</td>\n",
       "      <td>5.487273</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>today i cried over the lawn i am a woman who k...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>1582</td>\n",
       "      <td>319</td>\n",
       "      <td>4.943750</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>88</td>\n",
       "      <td>75</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>today makes 9 years sober  i never thought it ...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>1834</td>\n",
       "      <td>352</td>\n",
       "      <td>5.195467</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today i touched someone nothing sexual  in cas...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>483</td>\n",
       "      <td>93</td>\n",
       "      <td>5.138298</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i not only finished my 31 page paper  but i go...</td>\n",
       "      <td>offmychest</td>\n",
       "      <td>223</td>\n",
       "      <td>48</td>\n",
       "      <td>4.551020</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text       group  char_count  \\\n",
       "0  please be wary of people asking for donations ...  offmychest        1509   \n",
       "1  today i cried over the lawn i am a woman who k...  offmychest        1582   \n",
       "2  today makes 9 years sober  i never thought it ...  offmychest        1834   \n",
       "3  today i touched someone nothing sexual  in cas...  offmychest         483   \n",
       "4  i not only finished my 31 page paper  but i go...  offmychest         223   \n",
       "\n",
       "   word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0         274      5.487273                 47                 0   \n",
       "1         319      4.943750                 34                 0   \n",
       "2         352      5.195467                 68                 0   \n",
       "3          93      5.138298                 12                 0   \n",
       "4          48      4.551020                 11                 0   \n",
       "\n",
       "   upper_case_word_count  noun_count  verb_count  adj_count  adv_count  \\\n",
       "0                      2          63          63         19         19   \n",
       "1                     25          88          75         22         17   \n",
       "2                     42          89          88         20         40   \n",
       "3                      6          23          19          5          9   \n",
       "4                      6          15          11          3          3   \n",
       "\n",
       "   pron_count  \n",
       "0          30  \n",
       "1          34  \n",
       "2          34  \n",
       "3           9  \n",
       "4           2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.group.values\n",
    "\n",
    "tk = text.Tokenizer(num_words=200000)\n",
    "\n",
    "max_len = 40\n",
    "tk.fit_on_texts(list(data.full_text.values.astype(str)))\n",
    "x = tk.texts_to_sequences(data.full_text.values.astype(str))\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "embeddings_index_large = pickle.load( open( \"embeddings.p\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word), 300))\n",
    "for word, i in tqdm(word_index):\n",
    "    embedding_vector = embeddings_index_large.get(q)\n",
    "        embedding_vector:\n",
    "        embedding_matrix[i, :] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "filter_length = 5\n",
    "num_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "input_1 = Input(shape=(40,))\n",
    "embedding_1 = Embedding(input_dim=len(word_index),\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_1)\n",
    "\n",
    "timedistributed_1 = TimeDistributed(Dense(300, activation='relu'))(embedding)\n",
    "lambda_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "input_2 = Input(shape=(40,))\n",
    "embedding_2 = Embedding(input_dim=len(word_index),\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_2)\n",
    "\n",
    "timedistributed_2 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed)\n",
    "\n",
    "\n",
    "\n",
    "input_3 = Input(shape=(40,))\n",
    "embedding_3 = Embedding(input_dim=len(word_index),\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_3)\n",
    "\n",
    "convolution_3 = Convolution1D(nb_filter=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_3)\n",
    "\n",
    "dropout_3 = Dropout(0.2)(convolution_3)\n",
    "convolution_3_2 = Convolution1D(filters=num_filter,\n",
    "                         filter=filter,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)()\n",
    "\n",
    "\n",
    "globalmaxpooling1d_3 = GlobalMaxPooling1D()\n",
    "\n",
    "dropout_3_2 = Dropout(0.2)(globalmaxpooling1d_3)\n",
    "\n",
    "dense_3 = Dense(300)(globalmaxpooling1d)\n",
    "\n",
    "dropout_3_3 = Dropout(0.2)(dense_3)\n",
    "batchnormalization_3 = BatchNormalization()(dropout_3_3)\n",
    "\n",
    "\n",
    "input_4 = Input(shape=(40,))\n",
    "embedding_4 = Embedding(input_dim=len(word_index),\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_1)\n",
    "\n",
    "convolution_4 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_4)\n",
    "\n",
    "dropout_4 = Dropout(0.2)()\n",
    "convolution_4_2 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_4)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_4 = GlobalMaxPooling1D()()\n",
    "\n",
    "dropout_4_2 = Dropout(0.2)(globalmaxpooling1d_4)\n",
    "\n",
    "dense_4 = Dense(300)(globalmaxpooling1d_4)\n",
    "\n",
    "dropout = Dropout(0.2)(dense_4)\n",
    "batchnormalization_4 = BatchNormalization()(dropout_4_3)\n",
    "\n",
    "input = Input(shape=(40,))\n",
    "embedding_5 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)()\n",
    "\n",
    "dropout_5 = Dropout(0.2)(embedding_5)\n",
    "ltsm_5 = LSTM(300)(dropout_5)\n",
    "dropout_5_2 = Dropout(0.2)(ltsm_5)\n",
    "\n",
    "input_6 = Input(shape=(40,))\n",
    "embedding_6 = Embedding(input_dim=len(word_index),\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_6)\n",
    "\n",
    "dropout_6 = Dropout(0.2)(embedding_6)\n",
    "ltsm_6 = LSTM(300)(dropout_6)\n",
    "dropout_6_2 = Dropout(0.2)(ltsm_6)\n",
    "\n",
    "\n",
    "merged = concatenate([lambda_1, \n",
    "                      lambda_2, \n",
    "                      batchnormalization_3, \n",
    "                      batchnormalization_4,\n",
    "                      dropout_5_2,\n",
    "                      dropout_6_2])\n",
    "\n",
    "m_dense = Dense(300)(merged)\n",
    "m_relu = PReLU()(m_dense)\n",
    "m_dropout = Dropout(0.2)(m_relu)\n",
    "m_batch = BatchNormalization()()\n",
    "\n",
    "m2_dense = Dense(300)(m_batch)\n",
    "m2_relu = PReLU()(m2_dense)\n",
    "m2_dropout = Dropout(0.2)(m2_relu)\n",
    "m2_batch = BatchNormalization()()\n",
    "\n",
    "\n",
    "m3_dense = Dense(300)(m2_batch)\n",
    "m3_relu = PReLU()(m3_dense)\n",
    "m3_dropout = Dropout(0.2)(m3_relu)\n",
    "m3_batch = BatchNormalization()()\n",
    "\n",
    "m4_dense = Dense(300)(m3_batch)\n",
    "m4_relu = PReLU()(m4_dense)\n",
    "m4_dropout = Dropout(0.2)(m4_relu)\n",
    "m4_batch = BatchNormalization()()\n",
    "\n",
    "m5_dense = Dense(300)(m4_batch)\n",
    "m5_relu = PReLU()(m_dense)\n",
    "m5_dropout = Dropout(0.2)(m_relu)\n",
    "m5_batch = BatchNormalization()()\n",
    "\n",
    "\n",
    "dense_out = Dense(1, activation='sigmoid')(m5_batch)\n",
    "\n",
    "# build and compile model\n",
    "model = Model(inputs=[input_1, \n",
    "                      input_2, \n",
    "                      input_3, \n",
    "                      input_4,\n",
    "                      input_5, \n",
    "                      input_6], outputs=[dense_out])\n",
    "\n",
    "model.compile(optimizers.Adam(), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "model.fit([x], y=y, batch_size=400, epochs=40,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
